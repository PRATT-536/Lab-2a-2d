{"metadata":{"_change_revision":206,"_is_fork":false,"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# Build Lab 2b Build and refine risk models\n\nBuild Lab 2b builds statistical models to predict the risk tolerance, or risk aversion, of an investor - and then uses machine learning to refine the models and results\n","metadata":{"_cell_guid":"83708667-4fdc-1563-7b3a-06b6575d2865"}},{"cell_type":"markdown","source":"## Content","metadata":{"tags":[]}},{"cell_type":"markdown","source":"* RECREATE RESULTS FROM BUILD LAB 2a\n* [1. Problem Definition](#0)\n* [2. Getting Started - Load Libraries and Dataset](#1)\n    * [2.1. Load Libraries](#1.1)    \n    * [2.2. Load Dataset](#1.2)\n* [3. Data Preparation and Feature Selection](#2)\n    * [3.1. Preparing the predicted variable](#2.1)    \n    * [3.2. Feature Selection-Limit the Feature Space](#2.2)\n* RUN CODE FOR BUILD LAB 2b\n* [4.Evaluate Algorithms and Models](#4)        \n    * [4.1. Train/Test Split](#4.1)\n    * [4.2. Test Options and Evaluation Metrics](#4.2)\n    * [4.3. Compare Models and Algorithms](#4.3)\n* [5. Model Tuning and Grid Search](#5)  \n* [6. Finalize the Model](#6)  \n    * [6.1. Results on test dataset](#6.1)\n    * [6.2. Feature Importance](#6.1)\n    * [6.2. Feature Intuition](#6.3)\n","metadata":{}},{"cell_type":"markdown","source":"<a id='0'></a>\n# FIRST, RECREATE RESULTS FROM BUILD LAB 2a\n# 1. Problem Definition","metadata":{}},{"cell_type":"markdown","source":"In the supervised regression framework used for this case study, the predicted variable\nis the “true” risk tolerance of an individual10 and the predictor variables are demo‐\ngraphic, financial and behavioral attributes of an individual\n\nFor this case study the data used is from survey of Consumer Finances which is conducted by the Federal Reserve\nBoard. The data source is : \nhttps://www.federalreserve.gov/econres/scf_2009p.htm","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# 2. Getting Started- Loading the data and python packages","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a id='1.1'></a>\n## 2.1. Loading the python packages","metadata":{}},{"cell_type":"code","source":"# First steps - Build Lab 2b\n\n# Load core libraries so Jupyter Notebook can run Build Lab 2b in the Binder virtual environment\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nimport copy \nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n# Add libraries containing pre-written model validation methods\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Add libraries containing pre-written regression algorithms\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n# from sklearn.neural_network import MLPRegressor\n\n# Add libraries for Machine Learning - Deep Learning Models\n\nimport keras\nimport tensorflow\n#import tensorflow-gpu\n#import tensorflow-estimator\nimport tensorboard\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.optimizers import SGD\n\n# Add libraries containing basic statistical methods\n\nimport statsmodels.api as sm\n\n# Add lbraries to save the variables, models, and results produced by the Python code in this Jupyter Notebook\n\nfrom pickle import dump\nfrom pickle import load","metadata":{"_cell_guid":"5d8fee34-f454-2642-8b06-ed719f0317e1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.2'></a>\n## 2.2. Loading the Data","metadata":{}},{"cell_type":"code","source":"# Load panel dataset of investor characteristics\n\nos.chdir('../Data files for all Build Labs')\ndataset = pd.read_excel('SCFP2009panel.xlsx')\nos.chdir('../Build Lab 2b/')","metadata":{"_cell_guid":"787e35f7-bf9e-0969-8d13-a54fa87f3519","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Disable information messages\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n## 3. Data Preparation and Feature Selection","metadata":{}},{"cell_type":"markdown","source":"<a id='2.1'></a>\n## 3.1. Preparing the predicted variable","metadata":{}},{"cell_type":"markdown","source":"The dataset from \"Survey of Consumer Finances\" contains the Household's demographics, net worth, financial and non-financial assets for the same demographics in 2007 (pre-crisis) and 2009(post-crisis). \n\nWe prepare the predicted variable, which is the \"true\" risk tolerance in the following steps. There are different ways of getting the \"true\" risk tolerance. The idea and the purpose of this case study is to come up with an approach to solve the behavioral finance problem using machine learning. \n\nThe steps to compute the predicted variables are as follows: \n\n1) Compute the Risky asset and the riskless assets for all the individuals in the survey data. Risky and riskless assets are defined as follows: \n* **Risky assets** is investments in mutual funds, stocks, bonds, commodities, and\nreal estate, and an estimate of human capital. \n* **Risk Free Assets**: checking and savings balances,certificates of deposit, and other cash balances and equivalents.\n\n2) We take the ratio of risky assets to total assets of an investor and consider that as a measure of risk tolerance of an investor. From the data of SCF, we have the data of risky and riskless assets for the individuals for 2007 and 2009. We use this data and normalise the risky assets with the stock price of 2007 vs. 2009 to get risk tolerance. \n\n* **Risk Tolerance**  just defined as the ratio of Risky Asset to Riskless Assets normalised with the average S&P500 of 2007 vs 2009. \nAverage S&P500 in 2007: 1478\nAverage S&P500 in 2009: 948\n\n3) In a lot of literature, an intelligent investor is the one who doesn't change its risk tolerance during the change in the market. So, we consider the investors who change their risk tolerance by less than 10% between 2007 and 2009 as the intelligent investors. Ofcourse this is a qualitative judgement and is subject to change. However, as mentioned before more than being accurate and precise the purpose of theis case study is to demonstrate the usage of the machine learning and provide a machine learning based framework in behavioral finance and portfolio management which can be further leveraged for more detailed analysis. ","metadata":{}},{"cell_type":"code","source":"# Derive averages for SP500 during 2007 and 2009\n\nAverage_SP500_2007=1478\nAverage_SP500_2009=948\n\n# Risk Tolerance 2007\ndataset['RiskFree07']= dataset['LIQ07'] + dataset['CDS07'] + dataset['SAVBND07'] + dataset['CASHLI07']\ndataset['Risky07'] = dataset['NMMF07'] + dataset['STOCKS07'] + dataset['BOND07'] \ndataset['RT07'] = dataset['Risky07']/(dataset['Risky07']+dataset['RiskFree07'])\n\n# Risk Tolerance 2009\ndataset['RiskFree09']= dataset['LIQ09'] + dataset['CDS09'] + dataset['SAVBND09'] + dataset['CASHLI09']\ndataset['Risky09'] = dataset['NMMF09'] + dataset['STOCKS09'] + dataset['BOND09'] \ndataset['RT09'] = dataset['Risky09']/(dataset['Risky09']+dataset['RiskFree09'])*\\\n                (Average_SP500_2009/Average_SP500_2007)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset2 = copy.deepcopy(dataset)  \ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us compute the percentage change in risk tolerance between 2007 and 2009. ","metadata":{}},{"cell_type":"code","source":"dataset2['PercentageChange'] = np.abs(dataset2['RT09']/dataset2['RT07']-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for the rows with null or nan values and removing them.","metadata":{}},{"cell_type":"code","source":"# Check for null values and removing the null values'''\nprint('Null Values =',dataset2.isnull().values.any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the rows containing NA\ndataset2=dataset2.dropna(axis=0)\n\ndataset2=dataset2[~dataset2.isin([np.nan, np.inf, -np.inf]).any(1)]\n\n#Checking for any null values and removing the null values'''\nprint('Null Values =',dataset2.isnull().values.any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us plot the risk tolerance of 2007 and 2009. ","metadata":{}},{"cell_type":"code","source":"sns.distplot(dataset2['RT07'], hist=True, kde=False, \n             bins=int(180/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the risk tolerance of 2007, we see that a significant number of individuals had risk tolerance close to one.Meaning the investment ws more skewed towards the risky assets as compared to the riskless assets. Now let us look at the risk tolerance of 2009.","metadata":{}},{"cell_type":"code","source":"sns.distplot(dataset2['RT09'], hist=True, kde=False, \n             bins=int(180/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset3 = copy.deepcopy(dataset2)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, the behavior of the individuals reversed in 2009 after crisis and majority of the investment was in risk free assets. Overall risk tolerance decreased, which is shown by majority of risk tolerance being close to 0 in 2009. \nIn the next step we pick the intelligent investors whose risk tolerance change between 2007 and 2009 was less than 10%","metadata":{}},{"cell_type":"code","source":"dataset3 = dataset3[dataset3['PercentageChange']<=.1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We assign the true risk tolerance as the average risk tolerance of these intelligent investors between 2007 and 2009. This is the predicted variable for this case study. The purpose would be to predict the true risk tolerance of an individuals given the demographic, financial and willingness to take risk related features. ","metadata":{}},{"cell_type":"code","source":"dataset3['TrueRiskTolerance'] = (dataset3['RT07'] + dataset3['RT09'])/2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us drop other labels which might not be needed for the prediction. ","metadata":{}},{"cell_type":"code","source":"dataset3.drop(labels=['RT07', 'RT09'], axis=1, inplace=True)\ndataset3.drop(labels=['PercentageChange'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2.2'></a>\n## 3.2. Feature Selection-Limit the Feature Space ","metadata":{}},{"cell_type":"markdown","source":"<a id='2.2.2'></a>\n### 3.2.2.  Features elimination","metadata":{}},{"cell_type":"markdown","source":"In order to filter the features further we do the following:\n1. Check the description in the Data Dictionary (https://www.federalreserve.gov/econres/files/codebk2009p.txt, https://www.federalreserve.gov/econresdata/scf/files/fedstables.macro.txt)and only keep the features that are intuitive\nThe description is as follows: \n\n\n* AGE: There are 6 age categories, where 1 represents age less than 35 and 6 represents age more than 75.\n* EDUC: There are 4 education categories, where 1 represents no high school and 4 represents college degree.\n* MARRIED: It represents marital status. There are two categories where 1 represents married and 2 represents unmarried. \n* OCCU: It represents occupation category. 1 represents managerial category and 4 represents unemployed.\n* KIDS: It represents number of kids. \n* NWCAT: It represents net worth category. There are 5 categories, where 1 net worth less than 25 percentile and 5 represents net worth more than 90th percentile. \n* INCCL: It represents income category. There are 5 categories, where 1 income less than 10,000 and 5 represents net worth more than 100,000\n* RISK: It represents the willingness to take risk on a scale of 1 to 4, where 1 represents highest level of willingness to take risk. \n\n2. Keep only the intuitive factors as of 2007 only and remove all the intermediate features and features related to 2009, as the variables of 2007 are the only ones required for predicting the risk tolerance.\n","metadata":{}},{"cell_type":"code","source":"keep_list2 = ['AGE07','EDCL07','MARRIED07','KIDS07','OCCAT107','INCOME07','RISK07','NETWORTH07','TrueRiskTolerance'\n]\n\ndrop_list2 = [col for col in dataset3.columns if col not in keep_list2]\n\ndataset3.drop(labels=drop_list2, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us look at the correlation among the features.","metadata":{}},{"cell_type":"code","source":"# Perform correlation analysis\ncorrelation = dataset3.corr()\nplt.figure(figsize=(15,15))\nplt.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Produce scatterplot matrix\nfrom pandas.plotting import scatter_matrix\nplt.figure(figsize=(15,15))\nscatter_matrix(dataset3,figsize=(12,12))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the correlation chart above, networth and income are positively correlated with the risk tolerance.\nWith more number of kids and marriage the risk tolerance decreases. As the willingness to take risk decreases the risk tolerance decreases. With age there is a positive relationship of the risk tolerance. \n\nAs per the paper \"Does Risk Tolerance Decrease With Age?(Hui Wang1,Sherman Hanna)\", Relative risk aversion decreases as people age (i.e., the proportion of net wealth invested in risky assets increases as people age) when other variables are held constant.Therefore, risk tolerance increases with age. \n\nSo, in summary all the variables and their relationship with risk tolerance seems intuitive. ","metadata":{}},{"cell_type":"markdown","source":"<a id='4'></a>\n# NOW - RUN CODE FOR BUILD LAB 2b\n# 4. Evaluate Algorithms and Models","metadata":{}},{"cell_type":"markdown","source":"Let us evaluate the algorithms and the models. ","metadata":{}},{"cell_type":"markdown","source":"<a id='4.1'></a>\n## 4.1. Train Test Split","metadata":{}},{"cell_type":"markdown","source":"Performing a train and test split in this step. ","metadata":{}},{"cell_type":"code","source":"# Separate validation dataset for use in subsequent steps\nY= dataset3[\"TrueRiskTolerance\"]\nX = dataset3.loc[:, dataset3.columns != 'TrueRiskTolerance']\n\n# scaler = StandardScaler().fit(X)\n# rescaledX = scaler.transform(X)\nvalidation_size = 0.2\nseed = 42\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.2'></a>\n## 4.2. Test Options and Evaluation Metrics\n","metadata":{}},{"cell_type":"code","source":"# Test options for regression\nnum_folds = 10\n\n# Scoring = 'neg_mean_squared_error'\n# Scoring ='neg_mean_absolute_error'\nscoring = 'r2'","metadata":{"_cell_guid":"5702bc31-06bf-8b6a-42de-366a6b3311a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4.3'></a>\n## 4.3. Compare Models and Algorithms","metadata":{}},{"cell_type":"markdown","source":"### Regression Models","metadata":{}},{"cell_type":"code","source":"# Spot check the algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\n\n# Neural Network\n# models.append(('MLP', MLPRegressor()))\n\n# Boosting methods\nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n\n# Bagging methods\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))","metadata":{"_cell_guid":"772802f7-f4e4-84ee-6377-6464ab2e5da4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-folds cross validation","metadata":{}},{"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n    # Converted mean square error to positive. Lower is better\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"_cell_guid":"a784ab4a-eb59-98cc-76cf-b55f382d057a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Algorithm comparison","metadata":{}},{"cell_type":"code","source":"# Compare algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(15,8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The non linear models perform better than the linear models, which means that a non linear relationship between the risk tolerance and the difference variables use to predict it. Given random forest regression is one of the best methods, we use it for further grid search. ","metadata":{}},{"cell_type":"markdown","source":"<a id='5'></a>\n# 5. Model Tuning and Grid Search","metadata":{}},{"cell_type":"markdown","source":"Given that the Random Forest is the best model, Grid Search is performed on Random Forest.","metadata":{"_cell_guid":"848ca488-b0fd-8e93-2e68-23d32c71d89c"}},{"cell_type":"code","source":"# Perform Grid search: RandomForestRegressor \n'''\nn_estimators : integer, optional (default=10)\n    The number of trees in the forest.\n'''\nparam_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\nmodel = RandomForestRegressor()\nkfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest with number of estimators 250, is the best model after grid search. ","metadata":{}},{"cell_type":"markdown","source":"<a id='6'></a>\n# 6. Finalise the Model","metadata":{}},{"cell_type":"markdown","source":"Finalize Model with best parameters found during tuning step.","metadata":{}},{"cell_type":"markdown","source":"<a id='6.1'></a>\n## 6.1. Results on the Test Dataset","metadata":{}},{"cell_type":"code","source":"# Prepare model\nmodel = RandomForestRegressor(n_estimators = 250)\nmodel.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\npredictions_train = model.predict(X_train)\nprint(r2_score(Y_train, predictions_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimate accuracy on validation set\n# Transform the validation dataset\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n#rescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(X_validation)\nprint(mean_squared_error(Y_validation, predictions))\nprint(r2_score(Y_validation, predictions))","metadata":{"_cell_guid":"f9725666-3c21-69d1-ddf6-45e47d982444","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the mean square error and R2 shown above for the test set, the results look good. ","metadata":{}},{"cell_type":"markdown","source":"<a id='6.2'></a>\n## 6.2. Feature Importance and Features Intuition","metadata":{}},{"cell_type":"markdown","source":"Looking at the details above Random forest be worthy of further study.\nLet us look into the Feature Importance of the RF model","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators= 250,n_jobs=-1)\nmodel.fit(X_train,Y_train)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n\n# Plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the chart above, income and networth followed by age and willingness to take risk are the key variables to decide the risk tolerance. These variables have been considered as the key variables to model the risk tolerance across several literature. ","metadata":{}},{"cell_type":"markdown","source":"<a id='6.3'></a>\n## 6.3. Save Model for Later Use","metadata":{}},{"cell_type":"code","source":"# Save the variables, model, and results. They will be used again in Build Labs c-d\n# Save Model Using Pickle\nfrom pickle import dump\nfrom pickle import load\n\nfilename = 'build_lab2b.sav' \ndump(model, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model from disk\n\nloaded_model = load(open(filename, 'rb'))\n\n# Estimate accuracy on validation set\npredictions = loaded_model.predict(X_validation)\nresult = mean_squared_error(Y_validation, predictions)\nprint(r2_score(Y_validation, predictions))\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Congratulations you have completed Build Lab 2b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Conclusion__:\n\nWe showed that machine learning models might be able to objectively\nanalyze the behavior of different investors in a changing market and attribute these\nchanges to variables involved in determining risk appetite. With an increase in the\nvolume of investor’s data and availability of rich machine learning infrastructure,\nsuch models might prove to be more useful.\n\nWe saw that there is a non-linear relationship between the variables and the risk tolerance. Income and net worth followed by age and willingness to take risk are the key variables to decide the risk tolerance. These variables have been considered as the key variables to model the risk tolerance across several literature.\n","metadata":{}}]}